{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 17:30:15.796978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-10 17:30:15.946893: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-10 17:30:16.523396: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 17:30:16.523441: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 17:30:16.523448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our data, spanish and english corpus and the non breaking prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' ms.', ' ph.', ' prof.', ' sr.', ' st.'],\n",
       " [' sra.', ' dr.', ' prof.', ' ing.', ' st.'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/isma/Proyectos/Text_processing/Traductor/Files/P85-Non-Breaking-Prefix.en\",\n",
    "        mode= \"r\", encoding=\"utf-8\") as f:\n",
    "    nonBreaking_en = f.read()\n",
    "\n",
    "with open(\"/home/isma/Proyectos/Text_processing/Traductor/Files/nonbreaking_prefix.es\",\n",
    "        mode= \"r\", encoding=\"utf-8\") as f:\n",
    "    nonBreaking_es = f.read()\n",
    "\n",
    "nonBreaking_en = [' ' + pref + '.' for pref in nonBreaking_en.split(\"\\n\")]\n",
    "nonBreaking_es = [' ' + pref + '.' for pref in nonBreaking_es.split(\"\\n\")]\n",
    "\n",
    "nonBreaking_en[-10:-5], nonBreaking_es[-10:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset un_pc (/home/isma/Proyectos/Text_processing/Traductor/BiggerData/./NewDataset/un_pc/en-es/1.0.0/1360070a820db42f7427f5a98416dd3a1c956ae069b994bf2ec0b83ae16dcaee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89bab8749184d57bbbf01aa2e53b8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_dataset = load_dataset(\"un_pc\", \"en-es\", cache_dir=\"./NewDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = new_dataset.get(\"train\")[0:4500000]\n",
    "train_dataset = train_dataset.get(\"translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_en = train_dataset[30].get(\"en\")\n",
    "element_es = train_dataset[30].get(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 300000\n",
      "\n",
      "Elementos procesados: 67500\n",
      "Tamaño del conjunto aproximado: 67500\n",
      "\n",
      "Elementos procesados: 135000\n",
      "Tamaño del conjunto aproximado: 135000\n",
      "\n",
      "Elementos procesados: 202500\n",
      "Tamaño del conjunto aproximado: 202500\n",
      "\n",
      "Elementos procesados: 270000\n",
      "Tamaño del conjunto aproximado: 270000\n",
      "\n",
      "Elementos procesados: 337500\n",
      "Tamaño del conjunto aproximado: 337500\n",
      "\n",
      "Elementos procesados: 405000\n",
      "Tamaño del conjunto aproximado: 405000\n",
      "\n",
      "Elementos procesados: 472500\n",
      "Tamaño del conjunto aproximado: 472500\n",
      "\n",
      "Elementos procesados: 540000\n",
      "Tamaño del conjunto aproximado: 540000\n",
      "\n",
      "Elementos procesados: 607500\n",
      "Tamaño del conjunto aproximado: 607500\n",
      "\n",
      "Elementos procesados: 675000\n",
      "Tamaño del conjunto aproximado: 675000\n",
      "\n",
      "Elementos procesados: 742500\n",
      "Tamaño del conjunto aproximado: 742500\n",
      "\n",
      "Elementos procesados: 810000\n",
      "Tamaño del conjunto aproximado: 810000\n",
      "\n",
      "Elementos procesados: 877500\n",
      "Tamaño del conjunto aproximado: 877500\n",
      "\n",
      "Elementos procesados: 945000\n",
      "Tamaño del conjunto aproximado: 945000\n",
      "\n",
      "Elementos procesados: 1012500\n",
      "Tamaño del conjunto aproximado: 1012500\n",
      "\n",
      "Elementos procesados: 1080000\n",
      "Tamaño del conjunto aproximado: 1080000\n",
      "\n",
      "Elementos procesados: 1147500\n",
      "Tamaño del conjunto aproximado: 1147500\n",
      "\n",
      "Elementos procesados: 1215000\n",
      "Tamaño del conjunto aproximado: 1215000\n",
      "\n",
      "Elementos procesados: 1282500\n",
      "Tamaño del conjunto aproximado: 1282500\n",
      "\n",
      "Elementos procesados: 1350000\n",
      "Tamaño del conjunto aproximado: 1350000\n",
      "\n",
      "Elementos procesados: 1417500\n",
      "Tamaño del conjunto aproximado: 1417500\n",
      "\n",
      "Elementos procesados: 1485000\n",
      "Tamaño del conjunto aproximado: 1485000\n",
      "\n",
      "Elementos procesados: 1552500\n",
      "Tamaño del conjunto aproximado: 1552500\n",
      "\n",
      "Elementos procesados: 1620000\n",
      "Tamaño del conjunto aproximado: 1620000\n",
      "\n",
      "Elementos procesados: 1687500\n",
      "Tamaño del conjunto aproximado: 1687500\n",
      "\n",
      "Elementos procesados: 1755000\n",
      "Tamaño del conjunto aproximado: 1755000\n",
      "\n",
      "Elementos procesados: 1822500\n",
      "Tamaño del conjunto aproximado: 1822500\n",
      "\n",
      "Elementos procesados: 1890000\n",
      "Tamaño del conjunto aproximado: 1890000\n",
      "\n",
      "Elementos procesados: 1957500\n",
      "Tamaño del conjunto aproximado: 1957500\n",
      "\n",
      "Elementos procesados: 2025000\n",
      "Tamaño del conjunto aproximado: 2025000\n",
      "\n",
      "Elementos procesados: 2092500\n",
      "Tamaño del conjunto aproximado: 2092500\n",
      "\n",
      "Elementos procesados: 2160000\n",
      "Tamaño del conjunto aproximado: 2160000\n",
      "\n",
      "Elementos procesados: 2227500\n",
      "Tamaño del conjunto aproximado: 2227500\n",
      "\n",
      "Elementos procesados: 2295000\n",
      "Tamaño del conjunto aproximado: 2295000\n",
      "\n",
      "Elementos procesados: 2362500\n",
      "Tamaño del conjunto aproximado: 2362500\n",
      "\n",
      "Elementos procesados: 2430000\n",
      "Tamaño del conjunto aproximado: 2430000\n",
      "\n",
      "Elementos procesados: 2497500\n",
      "Tamaño del conjunto aproximado: 2497500\n",
      "\n",
      "Elementos procesados: 2565000\n",
      "Tamaño del conjunto aproximado: 2565000\n",
      "\n",
      "Elementos procesados: 2632500\n",
      "Tamaño del conjunto aproximado: 2632500\n",
      "\n",
      "Elementos procesados: 2700000\n",
      "Tamaño del conjunto aproximado: 2700000\n",
      "\n",
      "Elementos procesados: 2767500\n",
      "Tamaño del conjunto aproximado: 2767500\n",
      "\n",
      "Elementos procesados: 2835000\n",
      "Tamaño del conjunto aproximado: 2835000\n",
      "\n",
      "Elementos procesados: 2902500\n",
      "Tamaño del conjunto aproximado: 2902500\n",
      "\n",
      "Elementos procesados: 2970000\n",
      "Tamaño del conjunto aproximado: 2970000\n",
      "\n",
      "Elementos procesados: 3037500\n",
      "Tamaño del conjunto aproximado: 3037500\n",
      "\n",
      "Elementos procesados: 3105000\n",
      "Tamaño del conjunto aproximado: 3105000\n",
      "\n",
      "Elementos procesados: 3172500\n",
      "Tamaño del conjunto aproximado: 3172500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "buffer = 4500\n",
    "maxlen = 22 \n",
    "\n",
    "def process_chunk(chunk, buffer, index):\n",
    "    count = 0\n",
    "    \n",
    "    list_en = []\n",
    "    list_es = []\n",
    "\n",
    "    micro_list1 = []    \n",
    "    micro_list2 = []\n",
    "\n",
    "    for idx, element in enumerate(chunk):\n",
    "        # Una vez que hemos llenado las pequeñas listas, pasamos a hacer una limpieza de todos los datos antes de añadirlo a nuestro conjunto más grande\n",
    "        if len(micro_list1) > buffer-1:\n",
    "            # Data cleaning\n",
    "            for prefix in nonBreaking_en:\n",
    "                for element1, element2 in zip(micro_list1, micro_list2):\n",
    "                    element1 = element1.replace(prefix, prefix + '$$$')\n",
    "                    element2 = element2.replace(prefix, prefix + '$$$')\n",
    "\n",
    "                    element1 = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", element1)\n",
    "                    element2 = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", element2)\n",
    "\n",
    "                    element1 = re.sub(r\"\\.\\$\\$\\$\", '', element1)\n",
    "                    element2 = re.sub(r\"\\.\\$\\$\\$\", '', element2)\n",
    "\n",
    "                    element1 = re.sub(r\"  +\", \" \", element1)\n",
    "                    element2 = re.sub(r\"  +\", \" \", element2)\n",
    "\n",
    "            # Incorporate micro sections to full list, and reset microsections\n",
    "            list_en = list_en + micro_list1\n",
    "            micro_list1 = []\n",
    "            \n",
    "            list_es = list_es + micro_list2\n",
    "            micro_list2 = []\n",
    "\n",
    "            # Print informativo para saber la progresión\n",
    "            if index == 1:\n",
    "                count += buffer\n",
    "                print(\"Elementos procesados: {}\".format(count*15) )\n",
    "                print(\"Tamaño del conjunto aproximado: {}\\n\".format(len(list_en*15)) ) \n",
    "\n",
    "        element1 = element.get(\"en\")                         \n",
    "        element2 = element.get(\"es\") \n",
    "\n",
    "        if len(element1.split(\" \")) & len(element2.split(\" \")) <= maxlen:\n",
    "            micro_list1.append(element.get(\"en\"))\n",
    "            micro_list2.append(element.get(\"es\"))\n",
    "\n",
    "    return [list_en, list_es]\n",
    "        \n",
    "\n",
    "chunks = []\n",
    "chunk_size = int(len(train_dataset) / 15)\n",
    "print(\"chunk_size: {}\\n\".format(chunk_size))\n",
    "\n",
    "for i in range(0, len(train_dataset), chunk_size):\n",
    "    chunks.append(train_dataset[i: i+chunk_size])\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:\n",
    "    futures = [executor.submit(process_chunk, chunk, buffer, idx) for idx, chunk in enumerate(chunks)]\n",
    "    concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['THIRD MEETING OF THE PARTIES TO THE',\n",
       "  'MONTREAL PROTOCOL ON SUBSTANCES',\n",
       "  'THAT DEPLETE THE OZONE LAYER',\n",
       "  'Nairobi, 1921 June 1991',\n",
       "  'REPORT OF THE THIRD MEETING OF THE PARTIES'],\n",
       " ['TERCERA REUNION DE LAS PARTES EN EL PROTOCOLO',\n",
       "  'DE MONTREAL RELATIVO A LAS SUSTANCIAS',\n",
       "  'QUE AGOTAN LA CAPA DE OZONO',\n",
       "  'Nairobi, 19 a 21 de junio de 1991',\n",
       "  'INFORME DE LA TERCERA REUNION DE LAS PARTES EN EL'],\n",
       " 3078000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_es = []\n",
    "final_list_en = []\n",
    "\n",
    "for future in futures:\n",
    "    final_list_en = final_list_en + future.result()[0]\n",
    "    final_list_es = final_list_es + future.result()[1]\n",
    "\n",
    "final_list_en[5:10], final_list_es[5:10], len(final_list_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:17<00:00,  2.35it/s]\n",
      "100%|██████████| 38/38 [00:20<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_en = \"\\n\".join(final_list_en)\n",
    "corpus_es = \"\\n\".join(final_list_es)\n",
    "\n",
    "# Añadimos $$$ (algo que no encontraremos en el corpues) después de los puntos de frases sin fin\n",
    "for prefix in tqdm(nonBreaking_en):\n",
    "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "\n",
    "# Eliminamos los marcadores $$$\n",
    "corpus_en = re.sub(r\"\\.\\$\\$\\$\", '', corpus_en)\n",
    "\n",
    "# Eliminamos espacios múltiples\n",
    "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "# Realizamos el proceso anterior con el corpus español\n",
    "for prefix in tqdm(nonBreaking_es):\n",
    "    corpus_es = corpus_es.replace(prefix, prefix + '$$$')\n",
    "corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_es)\n",
    "corpus_es = re.sub(r\"\\.\\$\\$\\$\", '', corpus_es)\n",
    "corpus_es = re.sub(r\"  +\", \" \", corpus_es)\n",
    "corpus_es = corpus_es.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_size = (2**14) \n",
    "\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=target_vocab_size, )\n",
    "\n",
    "tokenizer_es = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_es, target_vocab_size=target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=16508>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16510, 16510)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_en = tokenizer_en.vocab_size + 2  \n",
    "vocab_size_es = tokenizer_es.vocab_size + 2 \n",
    "\n",
    "vocab_size_en, vocab_size_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[vocab_size_en -2] + tokenizer_en.encode(sentence) + [vocab_size_en -1] for sentence in corpus_en]\n",
    "\n",
    "outputs = [[vocab_size_es -2] + tokenizer_es.encode(sentence) + [vocab_size_es -1] for sentence in corpus_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189128it [09:17, 339.10it/s]\n",
      "209562it [04:23, 796.79it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1667596"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 22\n",
    "\n",
    "idx_to_rm = [count for count, sentence in enumerate(inputs) if len(sentence) > max_length]\n",
    "\n",
    "for idx in tqdm(reversed(idx_to_rm)):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "\n",
    "idx_to_rm = [count for count, sentence in enumerate(outputs) if len(sentence) > max_length]\n",
    "\n",
    "for idx in tqdm(reversed(idx_to_rm)):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, \n",
    "    value=0, \n",
    "    padding=\"post\", \n",
    "    maxlen=max_length\n",
    "    )\n",
    "\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    outputs, \n",
    "    value=0, \n",
    "    padding=\"post\", \n",
    "    maxlen=max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 18:41:40.713721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:40.743178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:40.743363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:40.744010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-10 18:41:40.744536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:40.744688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:40.744812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:41.243231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:41.243408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:41.243544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 18:41:41.243656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9155 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:06:00.0, compute capability: 8.6\n",
      "2023-01-10 18:41:41.244949: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 146748448 exceeds 10% of free system memory.\n",
      "2023-01-10 18:41:41.401441: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 146748448 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 192\n",
    "buffer_size = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()            # Cacheamos el dataset para agilizar el acceso a los datos en el entrenamiento\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size)   # Dividir el dataset en batches\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # Ajustar caché para que el acceso a los bloques de datos sea más rápido"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de modelo\n",
    "\n",
    "## 1. Encoding posicional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tratamiento de los espacios vectoriales pares: $$PE_{pos, 2i} = sin(pos/10000^{2i / dmodel})$$\n",
    "tratamiento de los espacios vectoriales impares: $$PE_{pos, 2i + 1} = cos(pos/10000^{2i / dmodel})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):  # pos : (seq_length, 1), i: (1, d_model)\n",
    "        angles = 1 / np.power( 10000., (2*( i // 2 )) / np.float32(d_model) )\n",
    "        return pos * angles  # (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2] # max_length\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "\n",
    "        angles = self.get_angles(\n",
    "                            pos=np.arange(seq_length)[:, np.newaxis], \n",
    "                            i=np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model=d_model)\n",
    "\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])  # aplicamos sin a las filas impares\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])  # aplicamos cos a las filas impares\n",
    "\n",
    "        pos_encoding = angles[np.newaxis, ...]   # Añadimos una dimensión extra al inicio para poder hacer el tratamiento de batches\n",
    "\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)   # Combinamos las entradas con el encoding posicional\n",
    "                                                            # que debe convertirse en tensor para poder ser utilizado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mecanismo de atención\n",
    "### 2.1 Cálculo de la atención\n",
    "\n",
    "$$Attention(Q, K, V) = softmax \\ \\left( \\dfrac{QK^{T}}{\\sqrt{d_{k}}} \\right) V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    keys_dim_sqrt = tf.math.sqrt(keys_dim)\n",
    "\n",
    "    scaled_product = product / keys_dim_sqrt\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "\n",
    "    softmax_scaled_product = tf.nn.softmax(scaled_product, axis=-1)\n",
    "\n",
    "    attention = tf.matmul(softmax_scaled_product, values)\n",
    "\n",
    "    return attention "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creación del multihead attention\n",
    "\n",
    "Utilizando la función anterior para el cálculo de la atención, crearemos una función para calcular paralelamente múltiples mecanismos de atención en los diferentes subespacios vectoriales en los que dividimos las entradas para aumentar la complejidad  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, nb_projections):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_projections = nb_projections\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # d_model es el tamaño de la dimensión de embedding\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # nos aseguramos de que el tamaño de la dimensión de embedding sea divisible \n",
    "        # entre el número de espacios vectoriales elegidos para el multiHeadAttention\n",
    "        assert self.d_model % self.nb_projections == 0 \n",
    "\n",
    "        # d_projections es el tamaño de cada una de las subdivisiones resultantes de \n",
    "        # dividir nuestra dimensión de embedding en subespacios vectoriales\n",
    "        self.d_projections = self.d_model // self.nb_projections\n",
    "\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "\n",
    "    # Para dividir nuestra entrada en los diferentes espacios vectoriales \n",
    "    # para el multihead attention utilizaremos esta función\n",
    "    #\n",
    "    # forma inicial de antes de pasar por la función inputs: \n",
    "    #   - (batch_size, seq_length, d_model)\n",
    "\n",
    "    def split_projetcions(self, inputs, batch_size):\n",
    "        # shape es la variable que utilizaremos para definir la forma de las inputs después \n",
    "        # de ser divididas en subespacios vectoriales\n",
    "        shape = (batch_size, -1, self.nb_projections, self.d_projections)\n",
    "\n",
    "        # forma del input después del cambio: (batch_size, seq_length, nb_projections, d_projections)\n",
    "        splitted_inputs = tf.reshape(inputs, shape=shape)\n",
    "\n",
    "        # forma después de la permutación: (batch_size, nb_projections, seq_length, d_projections)\n",
    "        permute_splitted_inputs = tf.transpose(splitted_inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # el objeto retornado está dividido en batches con cada entrada de datos, cada entrada\n",
    "        # de datos está dividido en los subespacios vectoriales, en cada subespacio encontraremos\n",
    "        # la frase (sequence) y por último la porción de la dimensión de embedding del subespacio\n",
    "        return permute_splitted_inputs\n",
    "\n",
    "\n",
    "    # La función call contiene lo necesario para completar el mecanismo de atención, dónde procesamos \n",
    "    # individualmente Q K V en subespacios vectoriales con split_projections, después llamaremos a la\n",
    "    # función que hará el cálculo de la atención, después sólo nos queda devolverle la forma que tenía\n",
    "    # originalmente la entrada y enviarlo a la capa lineal final\n",
    "\n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "\n",
    "        # llamamos a las capas lineales para cada Q K V\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "\n",
    "\n",
    "        # utilizamos la función para dividir nuestra entrada de datos en los subespacios vectoriales\n",
    "        queries = self.split_projetcions(inputs=queries, batch_size=batch_size)\n",
    "        keys = self.split_projetcions(inputs=keys, batch_size=batch_size)\n",
    "        values = self.split_projetcions(inputs=values, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        # calculamos la attention en base al procesamiento individual de los subespacios de queries, \n",
    "        # keys y values, después le damos la forma necesaria para ser seguir el proceso, es decir:\n",
    "        #  - (batch_size, nb_projections, seq_length, d_projections)\n",
    "\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "\n",
    "        # \"concat_attention\" es la variable que tiene todos los subespacios vectoriales generados para \n",
    "        # ser procesados en conjunto (formando el MultiHeadAttention) siendo enviados a la última capa \n",
    "        # lineal siendo su forma final:\n",
    "        #  - (batch_size, seq_length, d_model)\n",
    "        # es decir, la misma forma que tenía antes del proceso de división en subespacios\n",
    "\n",
    "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder\n",
    "### 3.1 Creando la capa encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, FC_units, nb_projections, dropout):\n",
    "        super(encoderLayer, self).__init__()\n",
    "\n",
    "        self.FC_units = FC_units\n",
    "        self.nb_projections = nb_projections\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_projections)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.FC_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        outputs = self.dense_1(attention) \n",
    "        outputs = self.dense_2(outputs) \n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention) \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creamos el encoder utilizando la clase de capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_layers, FC_units, nb_projections, dropout, vocab_size, d_model, name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        \n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "        self.encoding_layers = [encoderLayer(FC_units=FC_units, nb_projections=nb_projections, dropout=dropout) for _ in range(nb_layers)]\n",
    "\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.encoding_layers[i](outputs, mask, training) \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder\n",
    "### 4.1 Clase decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, FC_units, nb_projections, dropout):\n",
    "        super(decoderLayer, self).__init__()\n",
    "\n",
    "        self.FC_units = FC_units\n",
    "        self.nb_projections = nb_projections\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_projections)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_projections)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.FC_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        attention2 = self.multi_head_attention_2(attention, encoder_outputs, encoder_outputs, mask_2)\n",
    "        attention2 = self.dropout_2(attention2, training)\n",
    "        attention2 = self.norm_2(attention2 + inputs)\n",
    "\n",
    "        outputs = self.dense_1(attention2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Creamos el decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_layers, FC_units, nb_projections, dropout, vocab_size, d_model, name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "\n",
    "        self.decoder_layers = [decoderLayer(FC_units, nb_projections, dropout) for _ in range(nb_layers)]\n",
    "\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.positional_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.decoder_layers[i](outputs, encoder_outputs, mask_1, mask_2, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos todo para crear el Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size_encoder, vocab_size_decoder, d_model, nb_layers, FC_units, nb_projections, dropout, name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = Encoder(nb_layers, FC_units, nb_projections, dropout, vocab_size_encoder, d_model)\n",
    "\n",
    "        self.decoder = Decoder(nb_layers, FC_units, nb_projections, dropout, vocab_size_decoder, d_model)\n",
    "\n",
    "        self.last_linear = layers.Dense(units=vocab_size_decoder, name=\"linear_output\")\n",
    "\n",
    "\n",
    "    def create_padding_mask(self, seq): # batch_size, seq_length\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "    # Para que el modelo no pueda ver palabras futuras, debemos crear una matriz triangular       | 0 | 1 | 1 | 1 |\n",
    "    # de manera que las palabras que queremos ocultar se vayan mostrando una por una según        | 0 | 0 | 1 | 1 |\n",
    "    # el algoritmo vaya avanzando en la secuencia, de manera que en la primero sólo tendremos     | 0 | 0 | 0 | 1 |\n",
    "    # acceso a la primera palabra y iremos desbloqueando las siguientes para evitar que el        | 0 | 0 | 0 | 0 |\n",
    "    # transformer haga trampas y cree conexiones que fuera del entrenamiento resultarían ser \n",
    "    # inútiles. Para crear la matriz utilizaremos la sublibrería de tensorflow linalg (linear algebra) para tratar\n",
    "    # los numeros que se encuentren en la diagonal superior y poder crear la matriz triangular. Las posiciones con \n",
    "    # un 0 serán visibles mientras que las que tengan un 1 tendrán máscara para no ser vistas por el modelo\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "\n",
    "\n",
    "    def call(self, encoder_inputs, decoder_inputs, training):\n",
    "        encoder_mask = self.create_padding_mask(encoder_inputs)\n",
    "\n",
    "        # decoder_mask_1                                                                            [8, 0, 35, 74, 0, 12]\n",
    "\n",
    "        # combinamos las masks de cada secuencia con la look ahead mask para evitar que el        | 0 | 1 | 1 | 1 | 1 | 1 | \n",
    "        # modelo vea palabras futuras, de manera que con una secuencia de cómo la siguiente:      | 0 | 1 | 1 | 1 | 1 | 1 |\n",
    "        # - [8, 0, 35, 74, 0, 12] nosotros obtendríamos la siguiente matriz en decoder:           | 0 | 1 | 0 | 1 | 1 | 1 |\n",
    "        #                                                                                         | 0 | 1 | 0 | 0 | 1 | 1 |\n",
    "        #                                                                                         | 0 | 1 | 0 | 0 | 1 | 1 |\n",
    "        #                                                                                         | 0 | 1 | 0 | 0 | 1 | 0 |\n",
    "        decoder_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(decoder_inputs),\n",
    "            self.create_look_ahead_mask(decoder_inputs) )\n",
    "\n",
    "        decoder_mask_2 = self.create_padding_mask(encoder_inputs)\n",
    "\n",
    "        encoder_outputs = self.encoder(encoder_inputs, encoder_mask, training)\n",
    "\n",
    "        decoder_outputs = self.decoder(decoder_inputs, encoder_outputs, decoder_mask_1, decoder_mask_2, training)\n",
    "\n",
    "        outputs = self.last_linear(decoder_outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "\n",
    "d_model = 512\n",
    "nb_layers = 6\n",
    "FC_units = 1024\n",
    "nb_projections = 8\n",
    "dropout_rate = 0.15\n",
    "\n",
    "transformer = Transformer(vocab_size_encoder=vocab_size_en,\n",
    "                          vocab_size_decoder=vocab_size_es,\n",
    "                          d_model=d_model,\n",
    "                          nb_layers=nb_layers, \n",
    "                          FC_units=FC_units,\n",
    "                          nb_projections=nb_projections,\n",
    "                          dropout=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "# From_logits=True, las salidas ya vienen en números salidos de una función softmax listos para calcular la loss\n",
    "# Reduction=\"none\", las pérdidas con se calculan haciendo la media del batch completo sinó cada muestra del batch individualmente \n",
    "\n",
    "# usaremos nuestra propia función de pérdida\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$learning\\_ rate = d_{model}^{-0.5} \\cdot min(step\\_ num^{-0.5}, \\ step\\_ num \\cdot warmup\\_ steps^{-1.5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule nos permite jugar con la variable step para variar el learning rate en función de step\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=3000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)  # rational sqrt, (step_num^{-0.5})\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./ckpt/max_l{}\".format(max_length)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Último checkpoint restaurado!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 1.4516, accuracy: 0.2483]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 1\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.9979, accuracy: 0.2992]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 2\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.9179, accuracy: 0.3106]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 3\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.8737, accuracy: 0.3173]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 4\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.8430, accuracy: 0.3220]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 5\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.8203, accuracy: 0.3256]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 6\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.8024, accuracy: 0.3285]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 7\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7882, accuracy: 0.3308]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 8\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7761, accuracy: 0.3327]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 9\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7656, accuracy: 0.3345]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 10\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7560, accuracy: 0.3361]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 11\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7476, accuracy: 0.3375]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 12\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7401, accuracy: 0.3387]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 13\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7335, accuracy: 0.3398]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 14\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7270, accuracy: 0.3410]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 15\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7211, accuracy: 0.3419]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 16\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7160, accuracy: 0.3429]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 17\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7106, accuracy: 0.3438]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 18\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7058, accuracy: 0.3445]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "epoch: 19\n",
      "[Total: 8686 / 8468 batches / 1625856 processed]\t[loss: 0.7016, accuracy: 0.3453]\t\n",
      "|||||||||||||||||||||||||||||||||||||||\n",
      "Guardando checkpoint para epoch 20 en ./ckpt/max_l22/ckpt-20\n",
      "Tiempo total de epoch 4154.939123392105 sec\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output # For Jupyter Notebook\n",
    "\n",
    "epochs = 20 \n",
    "lista_total = []\n",
    "proggress = False\n",
    "total = len(dataset)\n",
    "count = 0\n",
    "count_batch = 0\n",
    "sections = 40\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if proggress:\n",
    "        lista_total.append(proggress)\n",
    "    lista_total.append(\"epoch: {}\".format(epoch))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (encoder_inputs, targets)) in enumerate(dataset):\n",
    "        \n",
    "        decoder_inputs = targets[:, :-1]\n",
    "        decoder_outputs_real = targets[:, 1:]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(encoder_inputs, decoder_inputs, True)\n",
    "            loss = loss_function(decoder_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables)) \n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(decoder_outputs_real, predictions)\n",
    "\n",
    "        #if batch % 20 == 0:\n",
    "        #    print(\"Epoch: {}. Lote: {}. Pérdidas: {:.4f}. Precisión {:.4f}\".format(epoch+1, batch, train_loss.result(), train_accuracy.result()) ) \n",
    "\n",
    "        count_batch+=1\n",
    "        \n",
    "        if count_batch > (total / sections):\n",
    "            clear_output(wait=True) # For Jupyter Notebook\n",
    "            #os.system('clear')\n",
    "\n",
    "            processed = total/sections * (count+1)\n",
    "            proggress = \"[Total: {} / {} batches / {} processed]\\t[loss: {:.4f}, accuracy: {:.4f}]\\t\".format(int(total), int(processed), int(processed)*batch_size, \n",
    "                                                                                                             train_loss.result(), train_accuracy.result()\n",
    "                                                                                                             ) + \"\\n\" + \"|\"*(count+1)\n",
    "\n",
    "            for line in lista_total:\n",
    "                print(line)\n",
    "            print(proggress)\n",
    "\n",
    "            count+=1\n",
    "            count_batch = 0\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    count = 0\n",
    "\n",
    "    print(\"Guardando checkpoint para epoch {} en {}\".format(epoch + 1, ckpt_save_path))\n",
    "    print(\"Tiempo total de epoch {} sec\".format(time.time() - start))\n",
    "\n",
    "    count_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    input_sentence = \\\n",
    "        [vocab_size_en-2] + tokenizer_en.encode(input_sentence) + [vocab_size_en-1]\n",
    "    \n",
    "    encoder_inputs = tf.expand_dims(input_sentence, axis=0)\n",
    "\n",
    "    output = tf.expand_dims([vocab_size_es-2], axis=0)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions = transformer(encoder_inputs, output, False)   # predictions shape: (1, seq_length, vocab_size_es)\n",
    "\n",
    "        prediction = predictions[:, -1:, :]                         # nos quedamos con:\n",
    "                                                                   #  - todas las secuencias (en éste caso sólo 1)\n",
    "                                                                   #  - la última palabra usada del input\n",
    "                                                                   #  - el vector con todas las palabras en español y la probabilidad de cada una\n",
    "\n",
    "        # tomando el vector de palabras españolas, obtenemos el id de la que más probabilidad ha obtenido\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)   \n",
    "\n",
    "        # la frase ha terminado, no hay nada más que decir\n",
    "        if predicted_id == vocab_size_es-1:\n",
    "            return tf.squeeze(output, axis=0)   # eliminamos la primera dimensión, la del batch, para quedarnos únicamente con la predicción\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    # la frase ha terminado porque hemos alcanzado max_length\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "# la función evaluate nos devuelve los tokens de las palabras en castellano por lo que tenemos que traducirlas\n",
    "\n",
    "def translate(sentence):\n",
    "    # ya no es necesario usar tensores, por lo que ahora la trabajamos con numpy\n",
    "    output = evaluate(sentence).numpy()\n",
    "\n",
    "    predicted_sentence = tokenizer_es.decode(\n",
    "        [i for i in output if i < vocab_size_es-2]\n",
    "    )\n",
    "\n",
    "    print(\"Entrada: {}\".format(sentence))\n",
    "    print(\"Salida: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a problem we have to solve.\n",
      "Salida: Este es un problema que nos enfrentamos.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a problem we have to solve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a really powerful tool!\n",
      "Salida: Es un hecho que nos parece ser un hecho.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f41f80dd5e5e329c215f6b6e4fcb487fc0c512ad7bf9118874dd7db84e187ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
